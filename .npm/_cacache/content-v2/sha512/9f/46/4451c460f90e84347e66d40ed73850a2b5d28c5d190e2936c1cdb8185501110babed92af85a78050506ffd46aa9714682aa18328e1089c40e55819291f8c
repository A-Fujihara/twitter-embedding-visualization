{"_id":"gpt4all","_rev":"12-dad375ca88c06d1bc1f3a9928fed7c56","name":"gpt4all","dist-tags":{"latest":"4.0.0","alpha":"2.2.0"},"versions":{"1.0.0":{"name":"gpt4all","version":"1.0.0","description":"GPT4All Typescript package","main":"dist/gpt4all.js","types":"dist/gpt4all.d.ts","author":{"name":"Conner Swann","email":"conner@intuitivesystems.xyz"},"license":"MIT","private":false,"dependencies":{"axios":"^1.3.4","os":"^0.1.2","progress":"^2.0.3"},"devDependencies":{"@types/progress":"^2.0.5"},"scripts":{"build":"tsc","test":"echo \"Error: no test specified\" && exit 1"},"_id":"gpt4all@1.0.0","dist":{"shasum":"d45ce61a1a8d602f14afa521afc3bea75062265d","integrity":"sha512-fpT5F2kMReP8bQmzdxhPX7Rx6HI2Gvn/T/JqjuDl/JEYoSQWZOINiYgV+lsZjyerst0bb4fU3Rs6BORNhDxz/g==","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-1.0.0.tgz","fileCount":10,"unpackedSize":33998,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEQCICz7q2mqbCtbWuHNVbobcysUAjsc0fOG8hcmw2SlRt2AAiA6MkIP9IL9K3X3Z8+Lg+Z4uBmxZOIeEUhm2E+/Qn0VSg=="}],"npm-signature":"-----BEGIN PGP SIGNATURE-----\r\nVersion: OpenPGP.js v4.10.10\r\nComment: https://openpgpjs.org\r\n\r\nwsFzBAEBCAAGBQJkKn+QACEJED1NWxICdlZqFiEECWMYAoorWMhJKdjhPU1b\r\nEgJ2Vmp2Yw//SN9YIBA9gc3iFQ9Yf9nvE0q2H67KD3XnXTSKekWDsbd0CQEd\r\niOlYZdkinaukKn6VaDgD/nFYV/C9ZS4xWp7bXoBoN2n8vHex64Rr0EbpWlgx\r\nkVo20Nc0cKv6pQiNfroCUHxhMh3w5Yd1hcYm+yAdLDCVoE9y7ASdGSlP5bKP\r\nk+nyWvtdzCGvaWiY7SWRiOplmkMP1dN2qGJi6WjW6eoie2bPWi+pBzsAHl/3\r\nOxy4arlX2+SMKglArbaHnr5dj2q+jTdClp95MBHe+JXlyYBNtKH0WB/TM69A\r\n++0u0f+7Wntf++K02Dg8Yoc5zAt5Da6stt9kHn4DISoFsGTF2RBN/saEyVy2\r\nTPc6lhIucX7lMzxqNrIE4E2dCe3UEpEKnb2KM/wqnDaNDmeSa7xvJjwHJ2Kw\r\n++RW6oum86kDMN8iGX3p4qxUADYOpir0BWgF8abcAxe3dEky9Xujqml87N56\r\nMPIjgn7YoK0NAICkilzvTfNGjfeL4pZYlD3wq9DVVDdaDwjx2YIWM/WFl1i2\r\nqX7VUKm4tm9ukLk/wY3ugIHlUXjPF0KeCUuhOj4h6Sx43EvHRB8M4WfNiEiO\r\nJsGUBQzGqsu0CbAwucISnmC/FSodI8WYWOhg0TggBAXRSbatdTig4JCrILnR\r\nRLqIfoLiSGdJEIpLoCSJaPgjqVXmj3CBc24=\r\n=CCvX\r\n-----END PGP SIGNATURE-----\r\n"},"_npmUser":{"name":"yourbuddyconner","email":"me@connerswann.me"},"directories":{},"maintainers":[{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_1.0.0_1680506767972_0.7520855279716576"},"_hasShrinkwrap":false},"2.0.0-alpha":{"name":"gpt4all","version":"2.0.0-alpha","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","prebuild":"node scripts/prebuild.js","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section documentation --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"node scripts/docs.js"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","gitHead":"2007c17370ba6312f8e4c9e3155e3518ba59a011","readme":"# GPT4All Node.js API\n\n```sh\nyarn install gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [Documentation](#Documentation)\n\n### Chat Completion (alpha)\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(ll, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n### Embedding (alpha)\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(ll, \"Pain is inevitable, suffering optional\");\n```\n\n### API\n\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   [docs](./docs/api.md)\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n#### docs/\n\n*   Autogenerated documentation using the script `yarn docs:build`\n\n### Known Issues\n\n    * why your model may be spewing bull ðŸ’© \n        - The downloaded model is broken (just reinstall or download from official site)\n        - That's it so far\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help) \n    - Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n### Documentation\n","readmeFilename":"README.md","description":"```sh yarn install gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.0.0-alpha","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-jPdOW5a2JEi0kx0HQZMlKIpkrQ6rCPvjzL2qtwO5y495ULvUrmrbDogBT0xbvrYxIC7w8/clHhl3Ms6P7VRZyA==","shasum":"a007a5455675c64af19bc38962ea36afd7f28e34","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.0.0-alpha.tgz","fileCount":6,"unpackedSize":28232,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIBaV0L4ElRNfr2ySl9n8cndZfK7vincMXBIQnb3UxZ58AiEAgM8d1qs3oaXbqi+HyzRhG3l0HFIFsZOb0f0bqEPKzFo="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.0.0-alpha_1689788360988_0.6688518599852571"},"_hasShrinkwrap":false},"2.0.1-alpha":{"name":"gpt4all","version":"2.0.1-alpha","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","prebuild":"node scripts/prebuild.js","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section documentation --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"node scripts/docs.js"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","readme":"# GPT4All Node.js API\n\n```sh\nyarn install gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [Documentation](#Documentation)\n\n### Chat Completion (alpha)\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(ll, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n### Embedding (alpha)\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(ll, \"Pain is inevitable, suffering optional\");\n```\n\n### API\n\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   [docs](./docs/api.md)\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n#### docs/\n\n*   Autogenerated documentation using the script `yarn docs:build`\n\n### Known Issues\n\n    * why your model may be spewing bull ðŸ’© \n        - The downloaded model is broken (just reinstall or download from official site)\n        - That's it so far\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help) \n    - Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n### Documentation\n","readmeFilename":"README.md","gitHead":"8ff33b72d1444e6aa721d428788b4452270f6729","description":"```sh yarn install gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.0.1-alpha","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-BHidW95C5m0VFitQsZDt5c/VR6B0iqDC+1BE3iucGyoCO5eivvqT3XU/Jl91gZE/jcQtCRd1OHOWIUYkg3fyJg==","shasum":"6d6860aff6c454207ab3b1f220b89b3809cc3737","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.0.1-alpha.tgz","fileCount":57,"unpackedSize":34077637,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIQDZeaNvquRi3qbdBakoARnV+MxhUCdHJhykwkrcVqnbPgIgNbLr4Jz1sxshG/G2mSA+XRrplgTHQ1nNCIRQNQcb59U="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.0.1-alpha_1689789413335_0.29953435581842225"},"_hasShrinkwrap":false},"2.0.2-alpha":{"name":"gpt4all","version":"2.0.2-alpha","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","prebuild":"node scripts/prebuild.js","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section documentation --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"node scripts/docs.js"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","readme":"# GPT4All Node.js API\n\n```sh\nyarn install gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [Documentation](#Documentation)\n\n### Chat Completion (alpha)\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(ll, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n### Embedding (alpha)\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(ll, \"Pain is inevitable, suffering optional\");\n```\n\n### API\n\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   [docs](./docs/api.md)\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n#### docs/\n\n*   Autogenerated documentation using the script `yarn docs:build`\n\n### Known Issues\n\n    * why your model may be spewing bull ðŸ’© \n        - The downloaded model is broken (just reinstall or download from official site)\n        - That's it so far\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help) \n    - Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n### Documentation\n","readmeFilename":"README.md","gitHead":"dc8fe40ea5644ba7c28a57537dcc748084a38ee3","description":"```sh yarn install gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.0.2-alpha","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-m8QZY6F4dbv9AXLkeaUHa/E3TlrMyYV1q6+hlzqXzmyZGBebMGRT8+L1e/ZMh5Bw6PTQHAEvz+35FZIhBerz6A==","shasum":"e85326546a1591a03337bf3417b0661efc23c307","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.0.2-alpha.tgz","fileCount":58,"unpackedSize":34079437,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIQDHh2DUibNM6RAEalJvpoTyCZ/8ujKT+237VtC1dLlgPAIgSmS9M7szetOOUK0+SC9xeZ33+gzPLKipAqyNtboDFLg="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.0.2-alpha_1689790234905_0.16439712199737166"},"_hasShrinkwrap":false},"2.1.0-alpha":{"name":"gpt4all","version":"2.1.0-alpha","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section documentation --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"node scripts/docs.js"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","readme":"# GPT4All Node.js API\n\n```sh\nyarn install gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes), [limez](https://github.com/iimez) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [Documentation](#Documentation)\n\n### Chat Completion (alpha)\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(ll, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n### Embedding (alpha)\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(ll, \"Pain is inevitable, suffering optional\");\n```\n\n### API\n\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   [docs](./docs/api.md)\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n#### docs/\n\n*   Autogenerated documentation using the script `yarn docs:build`\n\n### Known Issues\n\n    * why your model may be spewing bull ðŸ’© \n        - The downloaded model is broken (just reinstall or download from official site)\n        - That's it so far\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help) \n    - Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n### Documentation\n","readmeFilename":"README.md","gitHead":"8e4d1d56865b4ab8eeab0b40eb635dc26f600638","description":"```sh yarn install gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.1.0-alpha","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-+d75D6xuc2s0+ClBicZSvHo161ntvh5CijiXpGlx1Yoxo6hlm/sD+qUz+08hB/XJKVYsn2PSgPHDUqTwMY17Lg==","shasum":"001e6d17ea569f271e3ad268dd739ed02b964ad4","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.1.0-alpha.tgz","fileCount":71,"unpackedSize":34846464,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEMCH3+Wd9A0LeOavDgQlvq39WO/4DXvKZWEE2Y47qNqYYgCIEj9PWemsedzUTYc9MODFUKdQpBZfnwxtmQ0YO8U9FU7"}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.1.0-alpha_1690066004449_0.5666984619652329"},"_hasShrinkwrap":false},"2.1.1-alpha":{"name":"gpt4all","version":"2.1.1-alpha","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section documentation --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"node scripts/docs.js"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","readme":"# GPT4All Node.js API\n\n```sh\nyarn add gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes), [limez](https://github.com/iimez) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [Documentation](#Documentation)\n\n### Chat Completion (alpha)\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(ll, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n### Embedding (alpha)\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst ll = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(ll, \"Pain is inevitable, suffering optional\");\n```\n\n### API\n\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   [docs](./docs/api.md)\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n#### docs/\n\n*   Autogenerated documentation using the script `yarn docs:build`\n\n### Known Issues\n\n    * why your model may be spewing bull ðŸ’© \n        - The downloaded model is broken (just reinstall or download from official site)\n        - That's it so far\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help) \n    - Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n### Documentation\n","readmeFilename":"README.md","gitHead":"0e866a0e8fc5461a906f8dcd29f5a644fd06d3c7","description":"```sh yarn add gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.1.1-alpha","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-Ehn/+K+dNqdjmzgsB2UhbRKk8SwEWWVVJ4DT0hEZbvrv6MRHuxN9y+GyG5t/F1ZmfrLYf2e1tshv1oScPYHqew==","shasum":"b3d046d23943444501043ad37c4096014cc679c8","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.1.1-alpha.tgz","fileCount":71,"unpackedSize":34847888,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIB7tDLC2qZVZl5t0X2GEfJw1Z1dOp8AL5hTn0ZP9o7MiAiEA7FwoLTt0lu4UcAKWdqz1gW1ifE4FyG3TS9T0xJcCQuk="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.1.1-alpha_1690428228262_0.29899974491904224"},"_hasShrinkwrap":false},"2.2.0":{"name":"gpt4all","version":"2.2.0","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file README.md"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"types":"./src/gpt4all.d.ts","readme":"# GPT4All Node.js API\n\n```sh\nyarn add gpt4all@alpha\n\nnpm install gpt4all@alpha\n\npnpm install gpt4all@alpha\n```\n\nThe original [GPT4All typescript bindings](https://github.com/nomic-ai/gpt4all-ts) are now out of date.\n\n*   New bindings created by [jacoobes](https://github.com/jacoobes), [limez](https://github.com/iimez) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   The nodejs api has made strides to mirror the python api. It is not 100% mirrored, but many pieces of the api resemble its python counterpart.\n*   Everything should work out the box.\n*   See [API Reference](#api-reference)\n\n### Chat Completion\n\n```js\nimport { createCompletion, loadModel } from '../src/gpt4all.js'\n\nconst model = await loadModel('ggml-vicuna-7b-1.1-q4_2', { verbose: true });\n\nconst response = await createCompletion(model, [\n    { role : 'system', content: 'You are meant to be annoying and unhelpful.'  },\n    { role : 'user', content: 'What is 1 + 1?'  } \n]);\n\n```\n\n### Embedding\n\n```js\nimport { createEmbedding, loadModel } from '../src/gpt4all.js'\n\nconst model = await loadModel('ggml-all-MiniLM-L6-v2-f16', { verbose: true });\n\nconst fltArray = createEmbedding(model, \"Pain is inevitable, suffering optional\");\n```\n\n### Build Instructions\n\n*   binding.gyp is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW works as well to build the gpt4all-backend. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nyarn\n```\n\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --depth 1 --recursive\n```\n\n**AS OF NEW BACKEND** to build the backend,\n\n```sh\nyarn build:backend\n```\n\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native The only current way to use them is to put them in the current working directory of your application. That is, **WHEREVER YOU RUN YOUR NODE APPLICATION**\n\n*   llama-xxxx.dll is required.\n*   According to whatever model you are using, you'll need to select the proper model loader.\n    *   For example, if you running an Mosaic MPT model, you will need to select the mpt-(buildvariant).(dynamiclibrary)\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n### Known Issues\n\n*   why your model may be spewing bull ðŸ’©\n    *   The downloaded model is broken (just reinstall or download from official site)\n    *   That's it so far\n\n### Roadmap\n\nThis package is in active development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[ ] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help)\n    *   Should include prebuilds to avoid painful node-gyp errors\n*   \\[ ] createChatSession ( the python equivalent to create\\_chat\\_session )\n\n### API Reference\n\n<!-- Generated by documentation.js. Update this documentation by updating the source code. -->\n\n##### Table of Contents\n\n*   [ModelType](#modeltype)\n*   [ModelFile](#modelfile)\n    *   [gptj](#gptj)\n    *   [llama](#llama)\n    *   [mpt](#mpt)\n    *   [replit](#replit)\n*   [type](#type)\n*   [LLModel](#llmodel)\n    *   [constructor](#constructor)\n        *   [Parameters](#parameters)\n    *   [type](#type-1)\n    *   [name](#name)\n    *   [stateSize](#statesize)\n    *   [threadCount](#threadcount)\n    *   [setThreadCount](#setthreadcount)\n        *   [Parameters](#parameters-1)\n    *   [raw\\_prompt](#raw_prompt)\n        *   [Parameters](#parameters-2)\n    *   [embed](#embed)\n        *   [Parameters](#parameters-3)\n    *   [isModelLoaded](#ismodelloaded)\n    *   [setLibraryPath](#setlibrarypath)\n        *   [Parameters](#parameters-4)\n    *   [getLibraryPath](#getlibrarypath)\n*   [loadModel](#loadmodel)\n    *   [Parameters](#parameters-5)\n*   [createCompletion](#createcompletion)\n    *   [Parameters](#parameters-6)\n*   [createEmbedding](#createembedding)\n    *   [Parameters](#parameters-7)\n*   [CompletionOptions](#completionoptions)\n    *   [verbose](#verbose)\n    *   [systemPromptTemplate](#systemprompttemplate)\n    *   [promptTemplate](#prompttemplate)\n    *   [promptHeader](#promptheader)\n    *   [promptFooter](#promptfooter)\n*   [PromptMessage](#promptmessage)\n    *   [role](#role)\n    *   [content](#content)\n*   [prompt\\_tokens](#prompt_tokens)\n*   [completion\\_tokens](#completion_tokens)\n*   [total\\_tokens](#total_tokens)\n*   [CompletionReturn](#completionreturn)\n    *   [model](#model)\n    *   [usage](#usage)\n    *   [choices](#choices)\n*   [CompletionChoice](#completionchoice)\n    *   [message](#message)\n*   [LLModelPromptContext](#llmodelpromptcontext)\n    *   [logitsSize](#logitssize)\n    *   [tokensSize](#tokenssize)\n    *   [nPast](#npast)\n    *   [nCtx](#nctx)\n    *   [nPredict](#npredict)\n    *   [topK](#topk)\n    *   [topP](#topp)\n    *   [temp](#temp)\n    *   [nBatch](#nbatch)\n    *   [repeatPenalty](#repeatpenalty)\n    *   [repeatLastN](#repeatlastn)\n    *   [contextErase](#contexterase)\n*   [createTokenStream](#createtokenstream)\n    *   [Parameters](#parameters-8)\n*   [DEFAULT\\_DIRECTORY](#default_directory)\n*   [DEFAULT\\_LIBRARIES\\_DIRECTORY](#default_libraries_directory)\n*   [DEFAULT\\_MODEL\\_CONFIG](#default_model_config)\n*   [DEFAULT\\_PROMT\\_CONTEXT](#default_promt_context)\n*   [DEFAULT\\_MODEL\\_LIST\\_URL](#default_model_list_url)\n*   [downloadModel](#downloadmodel)\n    *   [Parameters](#parameters-9)\n    *   [Examples](#examples)\n*   [DownloadModelOptions](#downloadmodeloptions)\n    *   [modelPath](#modelpath)\n    *   [verbose](#verbose-1)\n    *   [url](#url)\n    *   [md5sum](#md5sum)\n*   [DownloadController](#downloadcontroller)\n    *   [cancel](#cancel)\n    *   [promise](#promise)\n\n#### ModelType\n\nType of the model\n\nType: (`\"gptj\"` | `\"llama\"` | `\"mpt\"` | `\"replit\"`)\n\n#### ModelFile\n\nFull list of models available\n@deprecated These model names are outdated and this type will not be maintained, please use a string literal instead\n\n##### gptj\n\nList of GPT-J Models\n\nType: (`\"ggml-gpt4all-j-v1.3-groovy.bin\"` | `\"ggml-gpt4all-j-v1.2-jazzy.bin\"` | `\"ggml-gpt4all-j-v1.1-breezy.bin\"` | `\"ggml-gpt4all-j.bin\"`)\n\n##### llama\n\nList Llama Models\n\nType: (`\"ggml-gpt4all-l13b-snoozy.bin\"` | `\"ggml-vicuna-7b-1.1-q4_2.bin\"` | `\"ggml-vicuna-13b-1.1-q4_2.bin\"` | `\"ggml-wizardLM-7B.q4_2.bin\"` | `\"ggml-stable-vicuna-13B.q4_2.bin\"` | `\"ggml-nous-gpt4-vicuna-13b.bin\"` | `\"ggml-v3-13b-hermes-q5_1.bin\"`)\n\n##### mpt\n\nList of MPT Models\n\nType: (`\"ggml-mpt-7b-base.bin\"` | `\"ggml-mpt-7b-chat.bin\"` | `\"ggml-mpt-7b-instruct.bin\"`)\n\n##### replit\n\nList of Replit Models\n\nType: `\"ggml-replit-code-v1-3b.bin\"`\n\n#### type\n\nModel architecture. This argument currently does not have any functionality and is just used as descriptive identifier for user.\n\nType: [ModelType](#modeltype)\n\n#### LLModel\n\nLLModel class representing a language model.\nThis is a base class that provides common functionality for different types of language models.\n\n##### constructor\n\nInitialize a new LLModel.\n\n###### Parameters\n\n*   `path` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** Absolute path to the model file.\n\n<!---->\n\n*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** If the model file does not exist.\n\n##### type\n\neither 'gpt', mpt', or 'llama' or undefined\n\nReturns **([ModelType](#modeltype) | [undefined](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined))**&#x20;\n\n##### name\n\nThe name of the model.\n\nReturns **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;\n\n##### stateSize\n\nGet the size of the internal state of the model.\nNOTE: This state data is specific to the type of model you have created.\n\nReturns **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** the size in bytes of the internal state of the model\n\n##### threadCount\n\nGet the number of threads used for model inference.\nThe default is the number of physical cores your computer has.\n\nReturns **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** The number of threads used for model inference.\n\n##### setThreadCount\n\nSet the number of threads used for model inference.\n\n###### Parameters\n\n*   `newNumber` **[number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)** The new number of threads.\n\nReturns **void**&#x20;\n\n##### raw\\_prompt\n\nPrompt the model with a given input and optional parameters.\nThis is the raw output from model.\nUse the prompt function exported for a value\n\n###### Parameters\n\n*   `q` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The prompt input.\n*   `params` **Partial<[LLModelPromptContext](#llmodelpromptcontext)>** Optional parameters for the prompt context.\n*   `callback` **function (res: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)): void**&#x20;\n\nReturns **void** The result of the model prompt.\n\n##### embed\n\nEmbed text with the model. Keep in mind that\nnot all models can embed text, (only bert can embed as of 07/16/2023 (mm/dd/yyyy))\nUse the prompt function exported for a value\n\n###### Parameters\n\n*   `text` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;\n*   `q`  The prompt input.\n*   `params`  Optional parameters for the prompt context.\n\nReturns **[Float32Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Float32Array)** The result of the model prompt.\n\n##### isModelLoaded\n\nWhether the model is loaded or not.\n\nReturns **[boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)**&#x20;\n\n##### setLibraryPath\n\nWhere to search for the pluggable backend libraries\n\n###### Parameters\n\n*   `s` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;\n\nReturns **void**&#x20;\n\n##### getLibraryPath\n\nWhere to get the pluggable backend libraries\n\nReturns **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)**&#x20;\n\n#### loadModel\n\nLoads a machine learning model with the specified name. The defacto way to create a model.\nBy default this will download a model from the official GPT4ALL website, if a model is not present at given path.\n\n##### Parameters\n\n*   `modelName` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The name of the model to load.\n*   `options` **(LoadModelOptions | [undefined](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined))?** (Optional) Additional options for loading the model.\n\nReturns **[Promise](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise)<(InferenceModel | EmbeddingModel)>** A promise that resolves to an instance of the loaded LLModel.\n\n#### createCompletion\n\nThe nodejs equivalent to python binding's chat\\_completion\n\n##### Parameters\n\n*   `model` **InferenceModel** The language model object.\n*   `messages` **[Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[PromptMessage](#promptmessage)>** The array of messages for the conversation.\n*   `options` **[CompletionOptions](#completionoptions)** The options for creating the completion.\n\nReturns **[CompletionReturn](#completionreturn)** The completion result.\n\n#### createEmbedding\n\nThe nodejs moral equivalent to python binding's Embed4All().embed()\nmeow\n\n##### Parameters\n\n*   `model` **EmbeddingModel** The language model object.\n*   `text` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** text to embed\n\nReturns **[Float32Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Float32Array)** The completion result.\n\n#### CompletionOptions\n\n**Extends Partial\\<LLModelPromptContext>**\n\nThe options for creating the completion.\n\n##### verbose\n\nIndicates if verbose logging is enabled.\n\nType: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)\n\n##### systemPromptTemplate\n\nTemplate for the system message. Will be put before the conversation with %1 being replaced by all system messages.\nNote that if this is not defined, system messages will not be included in the prompt.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n##### promptTemplate\n\nTemplate for user messages, with %1 being replaced by the message.\n\nType: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)\n\n##### promptHeader\n\nThe initial instruction for the model, on top of the prompt\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n##### promptFooter\n\nThe last instruction for the model, appended to the end of the prompt.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### PromptMessage\n\nA message in the conversation, identical to OpenAI's chat message.\n\n##### role\n\nThe role of the message.\n\nType: (`\"system\"` | `\"assistant\"` | `\"user\"`)\n\n##### content\n\nThe message content.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### prompt\\_tokens\n\nThe number of tokens used in the prompt.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n#### completion\\_tokens\n\nThe number of tokens used in the completion.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n#### total\\_tokens\n\nThe total number of tokens used.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n#### CompletionReturn\n\nThe result of the completion, similar to OpenAI's format.\n\n##### model\n\nThe model used for the completion.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n##### usage\n\nToken usage report.\n\nType: {prompt\\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number), completion\\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number), total\\_tokens: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)}\n\n##### choices\n\nThe generated completions.\n\nType: [Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[CompletionChoice](#completionchoice)>\n\n#### CompletionChoice\n\nA completion choice, similar to OpenAI's format.\n\n##### message\n\nResponse message\n\nType: [PromptMessage](#promptmessage)\n\n#### LLModelPromptContext\n\nModel inference arguments for generating completions.\n\n##### logitsSize\n\nThe size of the raw logits vector.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### tokensSize\n\nThe size of the raw tokens vector.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### nPast\n\nThe number of tokens in the past conversation.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### nCtx\n\nThe number of tokens possible in the context window.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### nPredict\n\nThe number of tokens to predict.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### topK\n\nThe top-k logits to sample from.\nTop-K sampling selects the next token only from the top K most likely tokens predicted by the model.\nIt helps reduce the risk of generating low-probability or nonsensical tokens, but it may also limit\nthe diversity of the output. A higher value for top-K (eg., 100) will consider more tokens and lead\nto more diverse text, while a lower value (eg., 10) will focus on the most probable tokens and generate\nmore conservative text. 30 - 60 is a good range for most tasks.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### topP\n\nThe nucleus sampling probability threshold.\nTop-P limits the selection of the next token to a subset of tokens with a cumulative probability\nabove a threshold P. This method, also known as nucleus sampling, finds a balance between diversity\nand quality by considering both token probabilities and the number of tokens available for sampling.\nWhen using a higher value for top-P (eg., 0.95), the generated text becomes more diverse.\nOn the other hand, a lower value (eg., 0.1) produces more focused and conservative text.\nThe default value is 0.4, which is aimed to be the middle ground between focus and diversity, but\nfor more creative tasks a higher top-p value will be beneficial, about 0.5-0.9 is a good range for that.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### temp\n\nThe temperature to adjust the model's output distribution.\nTemperature is like a knob that adjusts how creative or focused the output becomes. Higher temperatures\n(eg., 1.2) increase randomness, resulting in more imaginative and diverse text. Lower temperatures (eg., 0.5)\nmake the output more focused, predictable, and conservative. When the temperature is set to 0, the output\nbecomes completely deterministic, always selecting the most probable next token and producing identical results\neach time. A safe range would be around 0.6 - 0.85, but you are free to search what value fits best for you.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### nBatch\n\nThe number of predictions to generate in parallel.\nBy splitting the prompt every N tokens, prompt-batch-size reduces RAM usage during processing. However,\nthis can increase the processing time as a trade-off. If the N value is set too low (e.g., 10), long prompts\nwith 500+ tokens will be most affected, requiring numerous processing runs to complete the prompt processing.\nTo ensure optimal performance, setting the prompt-batch-size to 2048 allows processing of all tokens in a single run.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### repeatPenalty\n\nThe penalty factor for repeated tokens.\nRepeat-penalty can help penalize tokens based on how frequently they occur in the text, including the input prompt.\nA token that has already appeared five times is penalized more heavily than a token that has appeared only one time.\nA value of 1 means that there is no penalty and values larger than 1 discourage repeated tokens.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### repeatLastN\n\nThe number of last tokens to penalize.\nThe repeat-penalty-tokens N option controls the number of tokens in the history to consider for penalizing repetition.\nA larger value will look further back in the generated text to prevent repetitions, while a smaller value will only\nconsider recent tokens.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n##### contextErase\n\nThe percentage of context to erase if the context window is exceeded.\n\nType: [number](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number)\n\n#### createTokenStream\n\nTODO: Help wanted to implement this\n\n##### Parameters\n\n*   `llmodel` **[LLModel](#llmodel)**&#x20;\n*   `messages` **[Array](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array)<[PromptMessage](#promptmessage)>**&#x20;\n*   `options` **[CompletionOptions](#completionoptions)**&#x20;\n\nReturns **function (ll: [LLModel](#llmodel)): AsyncGenerator<[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)>**&#x20;\n\n#### DEFAULT\\_DIRECTORY\n\nFrom python api:\nmodels will be stored in (homedir)/.cache/gpt4all/\\`\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### DEFAULT\\_LIBRARIES\\_DIRECTORY\n\nFrom python api:\nThe default path for dynamic libraries to be stored.\nYou may separate paths by a semicolon to search in multiple areas.\nThis searches DEFAULT\\_DIRECTORY/libraries, cwd/libraries, and finally cwd.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### DEFAULT\\_MODEL\\_CONFIG\n\nDefault model configuration.\n\nType: ModelConfig\n\n#### DEFAULT\\_PROMT\\_CONTEXT\n\nDefault prompt context.\n\nType: [LLModelPromptContext](#llmodelpromptcontext)\n\n#### DEFAULT\\_MODEL\\_LIST\\_URL\n\nDefault model list url.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### downloadModel\n\nInitiates the download of a model file.\nBy default this downloads without waiting. use the controller returned to alter this behavior.\n\n##### Parameters\n\n*   `modelName` **[string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)** The model to be downloaded.\n*   `options` **DownloadOptions** to pass into the downloader. Default is { location: (cwd), verbose: false }.\n\n##### Examples\n\n```javascript\nconst download = downloadModel('ggml-gpt4all-j-v1.3-groovy.bin')\ndownload.promise.then(() => console.log('Downloaded!'))\n```\n\n*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** If the model already exists in the specified location.\n*   Throws **[Error](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error)** If the model cannot be found at the specified url.\n\nReturns **[DownloadController](#downloadcontroller)** object that allows controlling the download process.\n\n#### DownloadModelOptions\n\nOptions for the model download process.\n\n##### modelPath\n\nlocation to download the model.\nDefault is process.cwd(), or the current working directory\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n##### verbose\n\nDebug mode -- check how long it took to download in seconds\n\nType: [boolean](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean)\n\n##### url\n\nRemote download url. Defaults to `https://gpt4all.io/models/<modelName>`\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n##### md5sum\n\nMD5 sum of the model file. If this is provided, the downloaded file will be checked against this sum.\nIf the sums do not match, an error will be thrown and the file will be deleted.\n\nType: [string](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String)\n\n#### DownloadController\n\nModel download controller.\n\n##### cancel\n\nCancel the request to download if this is called.\n\nType: function (): void\n\n##### promise\n\nA promise resolving to the downloaded models config once the download is done\n\nType: [Promise](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise)\\<ModelConfig>\n","readmeFilename":"README.md","gitHead":"88322428cecfd3e6d48709b040780aa4650e4e1b","description":"```sh yarn add gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@2.2.0","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-xH1Cd1vPcDSYlGst5jJKIvdl+VMzUR7zGJ+14f9h5sDy6PRyuY9gsITz1BkDE10NV5M2+i2FiLIHg/OHJvdAog==","shasum":"c52dc7b0aa02b5f9e45a72a446e0dfa220294932","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-2.2.0.tgz","fileCount":78,"unpackedSize":39448425,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIBxn5NMYlvDsLdC7ZbUTG7Zlnvr4o1KsATsJNLOZmBpYAiEA7n1M/joRGnNdgALpfSpfX05ct0a7VNlsbojqAc+txxw="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_2.2.0_1692146345089_0.973066656550988"},"_hasShrinkwrap":false},"3.0.0":{"name":"gpt4all","version":"3.0.0","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","predocs:build":"node scripts/docs.js","docs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file ../python/docs/gpt4all_typescript.md","postdocs:build":"documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file README.md"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"publishConfig":{"registry":"https://registry.npmjs.org/","access":"public","tag":"latest"},"types":"./src/gpt4all.d.ts","gitHead":"661b5220bab788cfb8da3a18878eff08a1c8fa53","description":"```sh yarn add gpt4all@alpha","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@3.0.0","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-A+OjUYlNpOEph5g2iMAp2/Z/3oTq81JPG89QpEjQZ8hLgbtKGAMFj5hBkXZ3nBQYV6XuCDPP+63z2/s6H+m6mw==","shasum":"f8637872031a47dc0735a4fc30e3a1de0094c186","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-3.0.0.tgz","fileCount":41,"unpackedSize":27271197,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEYCIQD45oLXQ/n7E2OhISpi7LoWVuQ8FwHEcKQwAqdnNmp6AgIhAJ7H3k8RYZ9i17Uil4i7GGkxnGhwWYY1bUpvNDtK0nVu"}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_3.0.0_1698867478359_0.9564341475376708"},"_hasShrinkwrap":false},"3.1.0":{"name":"gpt4all","version":"3.1.0","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","docs:build":"node scripts/docs.js && documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file ../python/docs/gpt4all_nodejs.md"},"dependencies":{"md5-file":"^5.0.0","mkdirp":"^3.0.1","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"publishConfig":{"registry":"https://registry.npmjs.org/","access":"public","tag":"latest"},"types":"./src/gpt4all.d.ts","gitHead":"a1f27072c262dd3345d4140709a8629d365b120b","description":"Native Node.js LLM bindings for all.","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@3.1.0","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-c+4wdHMXdycp9jMjRESh6i/hJ3nbKWE/ZauZVMNb7NaUK1OhZBIB3W/QhUACrQEn0VgoDeb870cTwNh9PnqFEw==","shasum":"675d0a25d52fdec4d6d6b6adbda4cbba6049262a","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-3.1.0.tgz","fileCount":41,"unpackedSize":28399995,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEQCIFGXtH8FHiQqkYPwbaA8/6Bpdyx0V0U6T1IIOUoefV98AiBzh7+b+757iOK7SmfSik6Uoq7U/Hu7DlbSchugpX7aTA=="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_3.1.0_1702667389422_0.6892776567099006"},"_hasShrinkwrap":false},"4.0.0":{"name":"gpt4all","version":"4.0.0","packageManager":"yarn@3.6.1","main":"src/gpt4all.js","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"scripts":{"install":"node-gyp-build","test":"jest","build:backend":"node scripts/build.js","build":"node-gyp-build","docs:build":"node scripts/docs.js && documentation readme ./src/gpt4all.d.ts --parse-extension js d.ts --format md --section \"API Reference\" --readme-file ../python/docs/gpt4all_nodejs.md"},"dependencies":{"md5-file":"^5.0.0","node-addon-api":"^6.1.0","node-gyp-build":"^4.6.0","node-gyp":"9.x.x"},"devDependencies":{"@types/node":"^20.1.5","documentation":"^14.0.2","jest":"^29.5.0","prebuildify":"^5.0.1","prettier":"^2.8.8"},"optionalDependencies":{"node-gyp":"9.x.x"},"engines":{"node":">= 18.x.x"},"prettier":{"endOfLine":"lf","tabWidth":4},"jest":{"verbose":true},"publishConfig":{"registry":"https://registry.npmjs.org/","access":"public","tag":"latest"},"types":"./src/gpt4all.d.ts","gitHead":"55f3b056b7250be44ec3df7689081654652810a2","description":"Native Node.js LLM bindings for all.","bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"},"homepage":"https://github.com/nomic-ai/gpt4all#readme","_id":"gpt4all@4.0.0","_nodeVersion":"18.16.1","_npmVersion":"9.5.1","dist":{"integrity":"sha512-ArXOR/CU9fUVGSn7auUjs/oD2nnpbAWjA807svh3KGFTLM9PpuJs0r8werv7Pw+Jsl+kH1THcst8OXGSu/zjAA==","shasum":"44e0f91da9d757d331a55abe2534fcee569000f7","tarball":"https://registry.npmjs.org/gpt4all/-/gpt4all-4.0.0.tgz","fileCount":36,"unpackedSize":26642355,"signatures":[{"keyid":"SHA256:jl3bwswu80PjjokCgh0o2w5c2U4LhQAE57gj9cz1kzA","sig":"MEUCIQCRsIr473s0CaERKmYOQFkEsi603j44NhgQYPSAPZHP+QIgWoqc7eH92hXdNh2wGI4aDpswIXYebr+93Dx0FTngjNk="}]},"_npmUser":{"name":"jacoobes","email":"jacobnguyend@gmail.com"},"directories":{},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/gpt4all_4.0.0_1711643056893_0.4199457620865972"},"_hasShrinkwrap":false}},"time":{"created":"2023-04-03T07:26:07.971Z","1.0.0":"2023-04-03T07:26:08.278Z","modified":"2024-03-28T16:24:17.419Z","2.0.0-alpha":"2023-07-19T17:39:21.133Z","2.0.1-alpha":"2023-07-19T17:56:53.690Z","2.0.2-alpha":"2023-07-19T18:10:35.218Z","2.1.0-alpha":"2023-07-22T22:46:44.923Z","2.1.1-alpha":"2023-07-27T03:23:48.692Z","2.2.0":"2023-08-16T00:39:05.726Z","3.0.0":"2023-11-01T19:37:58.984Z","3.1.0":"2023-12-15T19:09:49.895Z","4.0.0":"2024-03-28T16:24:17.255Z"},"maintainers":[{"name":"jacoobes","email":"jacobnguyend@gmail.com"},{"name":"nomic_ben","email":"ben@nomic.ai"},{"name":"yourbuddyconner","email":"me@connerswann.me"}],"description":"Native Node.js LLM bindings for all.","readme":"# GPT4All Node.js API\n\nNative Node.js LLM bindings for all.\n\n```sh\nyarn add gpt4all@latest\n\nnpm install gpt4all@latest\n\npnpm install gpt4all@latest\n\n```\n## Breaking changes in version 4!!\n*   See [Transition](#changes)\n## Contents\n*   See [API Reference](#api-reference)\n*   See [Examples](#api-example)\n*   See [Developing](#develop)\n*   GPT4ALL nodejs bindings created by [jacoobes](https://github.com/jacoobes), [limez](https://github.com/iimez) and the [nomic ai community](https://home.nomic.ai), for all to use.\n*   [spare change](https://github.com/sponsors/jacoobes) for a college student? ðŸ¤‘\n## Api Examples\n### Chat Completion\n\nUse a chat session to keep context between completions. This is useful for efficient back and forth conversations.\n\n```js\nimport { createCompletion, loadModel } from \"../src/gpt4all.js\";\n\nconst model = await loadModel(\"orca-mini-3b-gguf2-q4_0.gguf\", {\n    verbose: true, // logs loaded model configuration\n    device: \"gpu\", // defaults to 'cpu'\n    nCtx: 2048, // the maximum sessions context window size.\n});\n\n// initialize a chat session on the model. a model instance can have only one chat session at a time.\nconst chat = await model.createChatSession({\n    // any completion options set here will be used as default for all completions in this chat session\n    temperature: 0.8,\n    // a custom systemPrompt can be set here. note that the template depends on the model.\n    // if unset, the systemPrompt that comes with the model will be used.\n    systemPrompt: \"### System:\\nYou are an advanced mathematician.\\n\\n\",\n});\n\n// create a completion using a string as input\nconst res1 = await createCompletion(chat, \"What is 1 + 1?\");\nconsole.debug(res1.choices[0].message);\n\n// multiple messages can be input to the conversation at once.\n// note that if the last message is not of role 'user', an empty message will be returned.\nawait createCompletion(chat, [\n    {\n        role: \"user\",\n        content: \"What is 2 + 2?\",\n    },\n    {\n        role: \"assistant\",\n        content: \"It's 5.\",\n    },\n]);\n\nconst res3 = await createCompletion(chat, \"Could you recalculate that?\");\nconsole.debug(res3.choices[0].message);\n\nmodel.dispose();\n```\n\n### Stateless usage\nYou can use the model without a chat session. This is useful for one-off completions.\n\n```js\nimport { createCompletion, loadModel } from \"../src/gpt4all.js\";\n\nconst model = await loadModel(\"orca-mini-3b-gguf2-q4_0.gguf\");\n\n// createCompletion methods can also be used on the model directly.\n// context is not maintained between completions.\nconst res1 = await createCompletion(model, \"What is 1 + 1?\");\nconsole.debug(res1.choices[0].message);\n\n// a whole conversation can be input as well.\n// note that if the last message is not of role 'user', an error will be thrown.\nconst res2 = await createCompletion(model, [\n    {\n        role: \"user\",\n        content: \"What is 2 + 2?\",\n    },\n    {\n        role: \"assistant\",\n        content: \"It's 5.\",\n    },\n    {\n        role: \"user\",\n        content: \"Could you recalculate that?\",\n    },\n]);\nconsole.debug(res2.choices[0].message);\n\n```\n\n### Embedding\n\n```js\nimport { loadModel, createEmbedding } from '../src/gpt4all.js'\n\nconst embedder = await loadModel(\"nomic-embed-text-v1.5.f16.gguf\", { verbose: true, type: 'embedding'})\n\nconsole.log(createEmbedding(embedder, \"Maybe Minecraft was the friends we made along the way\"));\n```\n\n### Streaming responses\n```js\nimport { loadModel, createCompletionStream } from \"../src/gpt4all.js\";\n\nconst model = await loadModel(\"mistral-7b-openorca.gguf2.Q4_0.gguf\", {\n    device: \"gpu\",\n});\n\nprocess.stdout.write(\"Output: \");\nconst stream = createCompletionStream(model, \"How are you?\");\nstream.tokens.on(\"data\", (data) => {\n    process.stdout.write(data);\n});\n//wait till stream finishes. We cannot continue until this one is done.\nawait stream.result;\nprocess.stdout.write(\"\\n\");\nmodel.dispose();\n\n```\n\n### Async Generators\n```js\nimport { loadModel, createCompletionGenerator } from \"../src/gpt4all.js\";\n\nconst model = await loadModel(\"mistral-7b-openorca.gguf2.Q4_0.gguf\");\n\nprocess.stdout.write(\"Output: \");\nconst gen = createCompletionGenerator(\n    model,\n    \"Redstone in Minecraft is Turing Complete. Let that sink in. (let it in!)\"\n);\nfor await (const chunk of gen) {\n    process.stdout.write(chunk);\n}\n\nprocess.stdout.write(\"\\n\");\nmodel.dispose();\n\n```\n### Offline usage\ndo this b4 going offline\n```sh\ncurl -L https://gpt4all.io/models/models3.json -o ./models3.json\n```\n```js\nimport { createCompletion, loadModel } from 'gpt4all'\n\n//make sure u downloaded the models before going offline!\nconst model = await loadModel('mistral-7b-openorca.gguf2.Q4_0.gguf', {\n    verbose: true,\n    device: 'gpu',\n    modelConfigFile: \"./models3.json\"\n});\n\nawait createCompletion(model, 'What is 1 + 1?', { verbose: true })\n\nmodel.dispose();\n```\n\n## Develop\n### Build Instructions\n\n*   `binding.gyp` is compile config\n*   Tested on Ubuntu. Everything seems to work fine\n*   Tested on Windows. Everything works fine.\n*   Sparse testing on mac os.\n*   MingW script works to build the gpt4all-backend. We left it there just in case. **HOWEVER**, this package works only with MSVC built dlls.\n\n### Requirements\n\n*   git\n*   [node.js >= 18.0.0](https://nodejs.org/en)\n*   [yarn](https://yarnpkg.com/)\n*   [node-gyp](https://github.com/nodejs/node-gyp)\n    *   all of its requirements.\n*   (unix) gcc version 12\n*   (win) msvc version 143\n    *   Can be obtained with visual studio 2022 build tools\n*   python 3\n*   On Windows and Linux, building GPT4All requires the complete Vulkan SDK. You may download it from here: https://vulkan.lunarg.com/sdk/home\n*   macOS users do not need Vulkan, as GPT4All will use Metal instead.\n\n### Build (from source)\n\n```sh\ngit clone https://github.com/nomic-ai/gpt4all.git\ncd gpt4all-bindings/typescript\n```\n\n*   The below shell commands assume the current working directory is `typescript`.\n\n*   To Build and Rebuild:\n\n```sh\nnode scripts/prebuild.js\n```\n*   llama.cpp git submodule for gpt4all can be possibly absent. If this is the case, make sure to run in llama.cpp parent directory\n\n```sh\ngit submodule update --init --recursive\n```\n\n```sh\nyarn build:backend\n```\nThis will build platform-dependent dynamic libraries, and will be located in runtimes/(platform)/native\n\n### Test\n\n```sh\nyarn test\n```\n\n### Source Overview\n\n#### src/\n\n*   Extra functions to help aid devex\n*   Typings for the native node addon\n*   the javascript interface\n\n#### test/\n\n*   simple unit testings for some functions exported.\n*   more advanced ai testing is not handled\n\n#### spec/\n\n*   Average look and feel of the api\n*   Should work assuming a model and libraries are installed locally in working directory\n\n#### index.cc\n\n*   The bridge between nodejs and c. Where the bindings are.\n\n#### prompt.cc\n\n*   Handling prompting and inference of models in a threadsafe, asynchronous way.\n\n### Known Issues\n\n*   why your model may be spewing bull ðŸ’©\n    *   The downloaded model is broken (just reinstall or download from official site)\n*   Your model is hanging after a call to generate tokens.\n    * Is `nPast` set too high? This may cause your model to hang (03/16/2024), Linux Mint, Ubuntu 22.04\n*  Your GPU usage is still high after node.js exits.\n    * Make sure to call `model.dispose()`!!!\n\n### Roadmap\n\nThis package has been stabilizing over time development, and breaking changes may happen until the api stabilizes. Here's what's the todo list:\n\n*   \\[ ] Purely offline. Per the gui, which can be run completely offline, the bindings should be as well.\n*   \\[ ] NPM bundle size reduction via optionalDependencies strategy (need help)\n    *   Should include prebuilds to avoid painful node-gyp errors\n*   \\[x] createChatSession ( the python equivalent to create\\_chat\\_session )\n*   \\[x] generateTokens, the new name for createTokenStream. As of 3.2.0, this is released but not 100% tested. Check spec/generator.mjs!\n*   \\[x] ~~createTokenStream, an async iterator that streams each token emitted from the model. Planning on following this [example](https://github.com/nodejs/node-addon-examples/tree/main/threadsafe-async-iterator)~~ May not implement unless someone else can complete\n*   \\[x] prompt models via a threadsafe function in order to have proper non blocking behavior in nodejs\n*   \\[x] generateTokens is the new name for this^\n*   \\[x] proper unit testing (integrate with circle ci)\n*   \\[x] publish to npm under alpha tag `gpt4all@alpha`\n*   \\[x] have more people test on other platforms (mac tester needed)\n*   \\[x] switch to new pluggable backend\n\n## Changes\nThis repository serves as the new bindings for nodejs users.\n- If you were a user of [these bindings](https://github.com/nomic-ai/gpt4all-ts), they are outdated.\n- Version 4 includes the follow breaking changes\n    * `createEmbedding` & `EmbeddingModel.embed()` returns an object, `EmbeddingResult`, instead of a float32array.\n    * Removed deprecated types `ModelType` and `ModelFile`\n    * Removed deprecated initiation of model by string path only\n\n\n### API Reference\n","readmeFilename":"README.md","users":{"flumpus-dev":true},"homepage":"https://github.com/nomic-ai/gpt4all#readme","repository":{"type":"git","url":"git+https://github.com/nomic-ai/gpt4all.git"},"bugs":{"url":"https://github.com/nomic-ai/gpt4all/issues"}}